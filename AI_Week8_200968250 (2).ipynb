{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Name: Anush Ravishankar\n",
        "#### Registration No: 200968250\n",
        "##### Week - 8 AI \n",
        "##### Section: A -- Roll No: 61"
      ],
      "metadata": {
        "id": "QXcHvvV8GHwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "env = gym.make(\"FrozenLake-v1\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Irsu7hFR9cOn",
        "outputId": "5dc1bb6f-676f-4d6d-fd6b-30a191fa90f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.9/dist-packages (0.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.9/dist-packages (from gym) (6.1.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.9/dist-packages (from gym) (1.22.4)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.9/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.8.0->gym) (3.15.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  Policy Iteration function with the following parameters \n",
        " - policy: 2D array of a size n(S) x n(A), each cell represents a probability of taking action 'a' in state 's'\n",
        " - environment: Initialized Open AI gym environment object\n",
        " - discount_factor: MDP discount factor\n",
        " - theta:  A  threshold  of  a  value  function  change.  Once  the  update  to  value function is lesser than this number\n",
        " - max_iterations: Maximum number of iterations"
      ],
      "metadata": {
        "id": "u5IAt7vwIytz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F56A-CuP9PoJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def policy_iteration(policy, env, discount_factor, theta, max_iterations):\n",
        "    # Get number of states and actions\n",
        "    num_states = env.observation_space.n\n",
        "    num_actions = env.action_space.n\n",
        "    \n",
        "    # Initialize value function with zeros\n",
        "    value_function = np.zeros(num_states)\n",
        "    \n",
        "    # Iterate until convergence or maximum number of iterations is reached\n",
        "    for i in range(max_iterations):\n",
        "        \n",
        "        # Policy Evaluation\n",
        "        while True:\n",
        "            # Initialize the maximum change in value function for the current iteration\n",
        "            delta = 0\n",
        "            \n",
        "            # Iterate over all states\n",
        "            for s in range(num_states):\n",
        "                # Save the old value for the current state\n",
        "                old_value = value_function[s]\n",
        "                \n",
        "                # Get the action probabilities for the current state according to the current policy\n",
        "                action_prob = policy[s]\n",
        "                \n",
        "                # Initialize the action values for the current state and action\n",
        "                action_values = np.zeros(num_actions)\n",
        "                \n",
        "                # Iterate over all actions for the current state\n",
        "                for a in range(num_actions):\n",
        "                    # Iterate over all possible transitions for the current state-action pair\n",
        "                    for prob, next_state, reward, done in env.P[s][a]:\n",
        "                        # Calculate the action value for the current state-action pair\n",
        "                        action_values[a] += prob * (reward + discount_factor * value_function[next_state])\n",
        "                \n",
        "                # Update the value function for the current state using the action values and action probabilities\n",
        "                value_function[s] = np.sum(action_prob * action_values)\n",
        "                \n",
        "                # Calculate the maximum change in value function for the current iteration\n",
        "                delta = max(delta, np.abs(old_value - value_function[s]))\n",
        "            \n",
        "            # If the maximum change in value function is less than the threshold theta, break the loop\n",
        "            if delta < theta:\n",
        "                break\n",
        "        \n",
        "        # Policy Improvement\n",
        "        policy_stable = True\n",
        "        \n",
        "        # Iterate over all states\n",
        "        for s in range(num_states):\n",
        "            # Save the old action for the current state\n",
        "            old_action = np.argmax(policy[s])\n",
        "            \n",
        "            # Initialize the action values for the current state and action\n",
        "            action_values = np.zeros(num_actions)\n",
        "            \n",
        "            # Iterate over all actions for the current state\n",
        "            for a in range(num_actions):\n",
        "                # Iterate over all possible transitions for the current state-action pair\n",
        "                for prob, next_state, reward, done in env.P[s][a]:\n",
        "                    # Calculate the action value for the current state-action pair\n",
        "                    action_values[a] += prob * (reward + discount_factor * value_function[next_state])\n",
        "            \n",
        "            # Choose the best action for the current state based on the action values\n",
        "            best_action = np.argmax(action_values)\n",
        "            \n",
        "            # If the old action is not equal to the new best action, set policy_stable to False\n",
        "            if old_action != best_action:\n",
        "                policy_stable = False\n",
        "            \n",
        "            # Update the policy for the current state based on the best action\n",
        "            policy[s] = np.eye(num_actions)[best_action]\n",
        "        \n",
        "        # If the policy is stable, break the loop\n",
        "        if policy_stable:\n",
        "            break\n",
        "    \n",
        "    # Return the final policy and value function\n",
        "    return policy, value_function\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Value Iteration function with the following parameters \n",
        " - environment: Initialized Open AI gym environment object \n",
        " - discount_factor: MDP discount factor \n",
        " - theta:  A  threshold  of  a  value  function  change.  Once  the  update  to  value function is below this number\n",
        " - max_iterations: Maximum number of iterations"
      ],
      "metadata": {
        "id": "drpiL_2yKIM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def value_iteration(env, discount_factor, theta, max_iterations):\n",
        "    # Get the number of states and actions from the environment\n",
        "    num_states = env.observation_space.n\n",
        "    num_actions = env.action_space.n\n",
        "    \n",
        "    # Initialize the value function with zeros for all states\n",
        "    value_function = np.zeros(num_states)\n",
        "    \n",
        "    # Iterate until the value function converges or the maximum number of iterations is reached\n",
        "    for i in range(max_iterations):\n",
        "        delta = 0\n",
        "        # For each state, find the action that maximizes the expected return\n",
        "        for s in range(num_states):\n",
        "            old_value = value_function[s]\n",
        "            action_values = np.zeros(num_actions)\n",
        "            # For each action, compute the expected return by summing over all possible next states and rewards\n",
        "            for a in range(num_actions):\n",
        "                for prob, next_state, reward, done in env.P[s][a]:\n",
        "                    action_values[a] += prob * (reward + discount_factor * value_function[next_state])\n",
        "            # Update the value function for the current state by taking the maximum over all actions\n",
        "            value_function[s] = np.max(action_values)\n",
        "            # Keep track of the maximum change in the value function\n",
        "            delta = max(delta, np.abs(old_value - value_function[s]))\n",
        "        # If the maximum change is less than the threshold, break out of the loop\n",
        "        if delta < theta:\n",
        "            break\n",
        "    \n",
        "    # Compute the policy by choosing the action that maximizes the expected return for each state\n",
        "    policy = np.zeros((num_states, num_actions))\n",
        "    for s in range(num_states):\n",
        "        action_values = np.zeros(num_actions)\n",
        "        for a in range(num_actions):\n",
        "            for prob, next_state, reward, done in env.P[s][a]:\n",
        "                action_values[a] += prob * (reward + discount_factor * value_function[next_state])\n",
        "        # Choose the action that maximizes the expected return\n",
        "        best_action = np.argmax(action_values)\n",
        "        # Update the policy with the chosen action\n",
        "        policy[s][best_action] = 1.0\n",
        "    \n",
        "    # Return the policy and the final value function\n",
        "    return policy, value_function\n",
        "\n"
      ],
      "metadata": {
        "id": "6l6eg4zg91jm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Compare  the \n",
        " - number of  wins\n",
        " - average  return  after  1000  episodes "
      ],
      "metadata": {
        "id": "Lf_tsAeLKUCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_episodes(policy, env, num_episodes):\n",
        "    total_reward = 0\n",
        "    num_wins = 0\n",
        "    for i in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        episode_reward = 0\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = np.random.choice(env.action_space.n, p=policy[state])\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            episode_reward += reward\n",
        "            state = next_state\n",
        "        total_reward += episode_reward\n",
        "        if episode_reward == 1:\n",
        "            num_wins += 1\n",
        "    return num_wins, total_reward / num_episodes\n",
        "\n",
        "n_states = env.observation_space.n\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "# Set the parameters\n",
        "discount_factor = 0.99\n",
        "theta = 1e-8\n",
        "max_iterations = 2000\n",
        "num_episodes = 1000\n",
        "\n",
        "# Run Policy Iteration\n",
        "policy = np.ones([n_states, n_actions]) / n_actions\n",
        "opt_policy, value_func = policy_iteration(policy, env, discount_factor, theta, max_iterations)\n",
        "num_wins_policy, avg_return_policy = run_episodes(opt_policy, env, num_episodes)\n",
        "\n",
        "# Run Value Iteration\n",
        "opt_policy, value_func = value_iteration(env, discount_factor, theta, max_iterations)\n",
        "num_wins_value, avg_return_value = run_episodes(opt_policy, env, num_episodes)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Policy Iteration: Number of wins = {num_wins_policy}, Average Return = {avg_return_policy}\")\n",
        "print(f\"Value Iteration: Number of wins = {num_wins_value}, Average Return = {avg_return_value}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hm8JDA7j-E-m",
        "outputId": "c683d987-4749-4055-c9e6-059e20aca995"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy Iteration: Number of wins = 746, Average Return = 0.746\n",
            "Value Iteration: Number of wins = 736, Average Return = 0.736\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference: \n",
        "#### The results show that both Policy Iteration and Value Iteration successfully learned the optimal policy for the FrozenLake-v1 environment, as evidenced by their high number of wins and average return close to 1.\n",
        "\n",
        "#### However, the Policy Iteration method performed slightly better in terms of the number of wins and average return. This could be because Policy Iteration directly optimizes the policy and updates the value function accordingly, while Value Iteration updates the value function directly and then extracts the policy from it. This may lead to suboptimal policies in some cases.\n",
        "\n",
        "#### Overall, both methods are effective for solving the FrozenLake-v1 environment, but Policy Iteration may be a slightly better choice for this specific problem."
      ],
      "metadata": {
        "id": "pYBcMIyMIQvs"
      }
    }
  ]
}