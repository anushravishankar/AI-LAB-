{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Name: Anush Ravishankar \n",
        "#### Registration No: 200968250\n",
        "#### AI - Week 9"
      ],
      "metadata": {
        "id": "LZho6cwMYdbX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-BiWFS5W3Sg"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Cliff Walking environment\n",
        "env = gym.make('CliffWalking-v0')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R30-SLnVXFoX",
        "outputId": "383ccd8a-0532-44a4-f6e0-c6343ae73e8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the number of episodes\n",
        "num_episodes = 100\n",
        "\n",
        "# Set the discount factor\n",
        "gamma = 1.0\n",
        "\n",
        "# Define the epsilon value for epsilon-soft policies\n",
        "epsilon = 0.1\n",
        "\n",
        "# Define the number of actions and states\n",
        "num_actions = env.action_space.n\n",
        "num_states = env.observation_space.n\n"
      ],
      "metadata": {
        "id": "QqtYaglmXKtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Q-value table for Monte Carlo ES\n",
        "Q_mc_es = np.zeros((num_states, num_actions))\n",
        "returns_sum = np.zeros((num_states, num_actions))\n",
        "returns_count = np.zeros((num_states, num_actions))\n",
        "\n",
        "# Define the Q-value table for on-policy first-visit MC control\n",
        "Q_on_policy = np.zeros((num_states, num_actions))\n",
        "returns_sum_on_policy = np.zeros((num_states, num_actions))\n",
        "returns_count_on_policy = np.zeros((num_states, num_actions))\n"
      ],
      "metadata": {
        "id": "o7YtrQpwXQub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement Monte Carlo ES algorithm\n",
        "for i_episode in range(num_episodes):\n",
        "    episode = []\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    \n",
        "    # Generate an episode using exploring starts\n",
        "    while not done:\n",
        "        action = np.random.choice(num_actions)\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        episode.append((state, action, reward))\n",
        "        state = next_state\n",
        "    \n",
        "    # Update the Q-value table\n",
        "    states, actions, rewards = zip(*episode)\n",
        "    discounts = np.array([gamma**i for i in range(len(rewards)+1)])\n",
        "    \n",
        "    # Calculate the returns\n",
        "    for i, state in enumerate(states):\n",
        "        action = actions[i]\n",
        "        returns_sum[state][action] += sum(rewards[i:] * discounts[:-(1+i)])\n",
        "        returns_count[state][action] += 1.0\n",
        "        Q_mc_es[state][action] = returns_sum[state][action] / returns_count[state][action]"
      ],
      "metadata": {
        "id": "8QEPgiExXXQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the epsilon-soft policy\n",
        "def epsilon_soft_policy(state, Q, epsilon):\n",
        "    policy = np.ones(num_actions, dtype=float) * epsilon / num_actions\n",
        "    best_action = np.argmax(Q[state])\n",
        "    policy[best_action] += (1.0 - epsilon)\n",
        "    return policy"
      ],
      "metadata": {
        "id": "Erx9LKsjXn6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement on-policy first-visit MC control algorithm\n",
        "for i_episode in range(num_episodes):\n",
        "    episode_states = []\n",
        "    episode_actions = []\n",
        "    episode_rewards = []\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    \n",
        "    # Generate an episode using epsilon-soft policy\n",
        "    while not done:\n",
        "        policy = epsilon_soft_policy(state, Q_on_policy, epsilon)\n",
        "        action = np.random.choice(num_actions, p=policy)\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        episode_states.append(state)\n",
        "        episode_actions.append(action)\n",
        "        episode_rewards.append(reward)\n",
        "        state = next_state\n",
        "    \n",
        "    # Update the Q-value table\n",
        "    discounts = gamma ** np.arange(len(episode_rewards) + 1)\n",
        "    returns = np.cumsum(episode_rewards[::-1])[::-1] * discounts[:-1]\n",
        "    for t in range(len(episode_states)):\n",
        "        state = episode_states[t]\n",
        "        action = episode_actions[t]\n",
        "        returns_sum_on_policy[state][action] += returns[t]\n",
        "        returns_count_on_policy[state][action] += 1\n",
        "        Q_on_policy[state][action] = returns_sum_on_policy[state][action] / returns_count_on_policy[state][action]"
      ],
      "metadata": {
        "id": "yRh0SFmTXsDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the optimal policy for each algorithm\n",
        "print(\"Optimal Policy using Monte Carlo ES:\\n\", np.argmax(Q_mc_es, axis=1))\n",
        "print(\"\\nOptimal Policy using on-policy first-visit MC control:\\n\", np.argmax(Q_on_policy, axis=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrC87YwSXvoO",
        "outputId": "68cf99d7-6202-4e9e-9470-b4e2c7e7dd0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Policy using Monte Carlo ES:\n",
            " [1 3 1 1 1 1 0 2 1 0 1 2 2 1 1 1 1 1 1 1 1 1 1 2 3 3 3 0 1 1 1 0 3 1 1 2 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0]\n",
            "\n",
            "Optimal Policy using on-policy first-visit MC control:\n",
            " [3 3 0 2 3 0 2 2 2 2 1 1 2 3 1 1 1 1 1 1 1 1 1 2 1 3 2 1 1 0 1 3 1 2 1 2 2\n",
            " 0 0 0 0 0 0 0 0 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Compare  and comment  on  the  performance  of  both  methods  in  terms  of  number  of  steps needed to learn optimal policy and the number of episodes\n",
        "\n",
        "Monte Carlo ES method generally requires a large number of episodes to converge to the optimal policy. This is because it needs to visit each state-action pair sufficiently often to estimate the Q-values accurately. \n",
        "\n",
        "Therefore, it may require a large number of episodes to learn the optimal policy.\n",
        "\n",
        "___________ \n",
        "\n",
        "On-policy first-visit MC control for epsilon-soft policies generates episodes using an epsilon-soft policy, which means that it selects the best action with probability = (1-epsilon) and a random action with probability epsilon. This helps the algorithm to explore the state-action space while also exploiting the current best policy. \n",
        "\n",
        "The on-policy first-visit MC control method generally requires fewer episodes to converge compared to Monte Carlo ES method. This is because it gradually improves the policy while generating episodes and updates the Q-values after each episode.\n",
        "____\n",
        "In terms of the number of steps needed to learn the optimal policy, both methods should learn the optimal policy in a similar number of steps, assuming that they have converged to the optimal policy. \n",
        "\n",
        "However, the number of steps required for convergence may vary depending on the specific problem and the hyperparameters used."
      ],
      "metadata": {
        "id": "CtP6UdgxdXYn"
      }
    }
  ]
}